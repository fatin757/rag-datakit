{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "omlzs2cvvvb",
   "metadata": {},
   "source": [
    "# Embedding Model Fine-tuning with Sentence Transformers\n",
    "\n",
    "[![Open In Colab](https://img.shields.io/badge/Open%20In-Colab-blue?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/dnth/rag-datakit/blob/main/nbs/02_train.ipynb)\n",
    "[![Open In Kaggle](https://img.shields.io/badge/Open%20In-Kaggle-blue?style=for-the-badge&logo=kaggle)](https://kaggle.com/kernels/welcome?src=https://github.com/dnth/rag-datakit/blob/main/nbs/02_train.ipynb)\n",
    "\n",
    "This notebook demonstrates how to fine-tune an embedding model using the synthetic triplet data generated from Singapore SkillsFuture Framework job descriptions. We'll use the sentence-transformers library to train a model that can better understand job-related semantic similarity for improved retrieval and matching.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to set up sentence-transformers training pipeline\n",
    "- Configuring training arguments for embedding models\n",
    "- Using MultipleNegativesRankingLoss for triplet training\n",
    "- Monitoring training with Weights & Biases\n",
    "- Saving and publishing trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16faeb0",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the rag-datakit package which includes all necessary dependencies including distilabel, transformers, and dataset utilities. Uncomment the cell below to install if you haven't already.\n",
    "\n",
    "On Google Colab you might need to uninstall the existing packages due to conflicting versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594642ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.55.2\n",
      "Uninstalling transformers-4.55.2:\n",
      "  Successfully uninstalled transformers-4.55.2\n",
      "Found existing installation: torch 2.8.0\n",
      "Uninstalling torch-2.8.0:\n",
      "  Successfully uninstalled torch-2.8.0\n",
      "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f0d98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dnth/rag-datakit.git\n",
      "  Cloning https://github.com/dnth/rag-datakit.git to /private/var/folders/n3/bmyftghn5h9gg0sv_h_mjkw40000gn/T/pip-req-build-r3nh2wq8\n",
      "  Running command git clone --quiet https://github.com/dnth/rag-datakit.git /private/var/folders/n3/bmyftghn5h9gg0sv_h_mjkw40000gn/T/pip-req-build-r3nh2wq8\n",
      "  Resolved https://github.com/dnth/rag-datakit.git to commit e7e791e3a187011848a6acb8744f123f98d6a4ae\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=1.10.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from rag-datakit==0.1.0) (1.10.0)\n",
      "Requirement already satisfied: datasets>=4.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from rag-datakit==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: distilabel>=1.5.3 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (1.5.3)\n",
      "Requirement already satisfied: ipywidgets>=8.1.7 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from rag-datakit==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: nbformat>=5.10.4 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from rag-datakit==0.1.0) (5.10.4)\n",
      "Requirement already satisfied: pillow>=11.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from rag-datakit==0.1.0) (11.3.0)\n",
      "Requirement already satisfied: sentence-transformers>=5.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from rag-datakit==0.1.0) (5.1.0)\n",
      "Collecting torch>=2.8.0 (from rag-datakit==0.1.0)\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting transformers>=4.55.0 (from rag-datakit==0.1.0)\n",
      "  Using cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: wandb>=0.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from rag-datakit==0.1.0) (0.21.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from accelerate>=1.10.0->rag-datakit==0.1.0) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from accelerate>=1.10.0->rag-datakit==0.1.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from accelerate>=1.10.0->rag-datakit==0.1.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from accelerate>=1.10.0->rag-datakit==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from accelerate>=1.10.0->rag-datakit==0.1.0) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from accelerate>=1.10.0->rag-datakit==0.1.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from datasets>=4.0.0->rag-datakit==0.1.0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from datasets>=4.0.0->rag-datakit==0.1.0) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from datasets>=4.0.0->rag-datakit==0.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from datasets>=4.0.0->rag-datakit==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from datasets>=4.0.0->rag-datakit==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from datasets>=4.0.0->rag-datakit==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from datasets>=4.0.0->rag-datakit==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from datasets>=4.0.0->rag-datakit==0.1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->rag-datakit==0.1.0) (2025.3.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: orjson>=3.10.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (3.11.1)\n",
      "Requirement already satisfied: portalocker>=2.8.2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (2.10.6)\n",
      "Requirement already satisfied: rich>=13.5.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (14.1.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (1.15.3)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (80.9.0)\n",
      "Requirement already satisfied: tblib>=3.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (0.16.0)\n",
      "Requirement already satisfied: universal-pathlib>=0.2.2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (0.2.6)\n",
      "Requirement already satisfied: openai>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (1.68.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipywidgets>=8.1.7->rag-datakit==0.1.0) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipywidgets>=8.1.7->rag-datakit==0.1.0) (9.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipywidgets>=8.1.7->rag-datakit==0.1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipywidgets>=8.1.7->rag-datakit==0.1.0) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipywidgets>=8.1.7->rag-datakit==0.1.0) (3.0.15)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from nbformat>=5.10.4->rag-datakit==0.1.0) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from nbformat>=5.10.4->rag-datakit==0.1.0) (4.25.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from nbformat>=5.10.4->rag-datakit==0.1.0) (5.7.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from sentence-transformers>=5.1.0->rag-datakit==0.1.0) (1.6.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from sentence-transformers>=5.1.0->rag-datakit==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from torch>=2.8.0->rag-datakit==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from transformers>=4.55.0->rag-datakit==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from transformers>=4.55.0->rag-datakit==0.1.0) (0.21.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from wandb>=0.21.1->rag-datakit==0.1.0) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from wandb>=0.21.1->rag-datakit==0.1.0) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from wandb>=0.21.1->rag-datakit==0.1.0) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from wandb>=0.21.1->rag-datakit==0.1.0) (6.31.1)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from wandb>=0.21.1->rag-datakit==0.1.0) (2.34.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->rag-datakit==0.1.0) (3.12.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.21.1->rag-datakit==0.1.0) (4.0.12)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from httpx>=0.25.2->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from httpx>=0.25.2->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from httpx>=0.25.2->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from httpx>=0.25.2->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.10.0->rag-datakit==0.1.0) (1.1.7)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from jinja2>=3.1.2->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from jsonschema>=2.6->nbformat>=5.10.4->rag-datakit==0.1.0) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from jsonschema>=2.6->nbformat>=5.10.4->rag-datakit==0.1.0) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from jsonschema>=2.6->nbformat>=5.10.4->rag-datakit==0.1.0) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from jsonschema>=2.6->nbformat>=5.10.4->rag-datakit==0.1.0) (0.27.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from openai>=1.0.0->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from openai>=1.0.0->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from openai>=1.0.0->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from pydantic>=2.0->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from pydantic>=2.0->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=4.0.0->rag-datakit==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=4.0.0->rag-datakit==0.1.0) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from rich>=13.5.0->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.8.0->rag-datakit==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from typer>=0.9.0->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from pandas->datasets>=4.0.0->rag-datakit==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from pandas->datasets>=4.0.0->rag-datakit==0.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from pandas->datasets>=4.0.0->rag-datakit==0.1.0) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from scikit-learn->sentence-transformers>=5.1.0->rag-datakit==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from scikit-learn->sentence-transformers>=5.1.0->rag-datakit==0.1.0) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->rag-datakit==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->rag-datakit==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->rag-datakit==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->rag-datakit==0.1.0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->rag-datakit==0.1.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->rag-datakit==0.1.0) (1.20.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.21.1->rag-datakit==0.1.0) (5.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=13.5.0->distilabel>=1.5.3->distilabel[hf-transformers,openai]>=1.5.3->rag-datakit==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0.0->rag-datakit==0.1.0) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8.1.7->rag-datakit==0.1.0) (0.2.3)\n",
      "Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Using cached transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "Installing collected packages: torch, transformers\n",
      "Successfully installed torch-2.8.0 transformers-4.55.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/dnth/rag-datakit.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jllnb3x8pe",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We begin by importing the essential libraries for training our embedding model:\n",
    "\n",
    "- **sentence_transformers**: The core library providing tools for training and evaluating sentence transformers\n",
    "- **datasets**: Hugging Face's library for loading and managing our training dataset\n",
    "- **wandb**: Weights & Biases for experiment tracking and visualization of training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52da3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cb44d",
   "metadata": {},
   "source": [
    "## Dataset Loading and Inspection\n",
    "\n",
    "We load the preprocessed training dataset with train/validation splits that were created in the previous notebook. This dataset contains triplets specifically formatted for embedding model training using contrastive learning approaches.\n",
    "\n",
    "**Dataset source**: `dnth/ssf-train-valid`  \n",
    "**Structure**:\n",
    "- **anchor**: Original job descriptions from the SkillsFuture Framework\n",
    "- **positive**: Semantically similar paraphrases generated through synthetic data techniques\n",
    "- **negative**: Semantically different job descriptions used as contrastive examples\n",
    "\n",
    "Let's examine the dataset structure and review a sample triplet to understand the data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a89eb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative'],\n",
       "        num_rows: 6032\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative'],\n",
       "        num_rows: 1508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Fatin757/ssf-train-valid\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab59509f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': 'The Chief Executive Officer/Chief Operating Officer/Managing Director/General Manager/President defines the long-term strategic direction to grow the business in line with the organisations overall vision, mission and values. He/She translates broad goals into achievable steps, anticipates and stays ahead of trends, and takes advantage of business opportunities. He represents the organisation with customers, investors, and business partners, and holds responsibility for fostering a culture of workplace safety and health and adherence to industry quality standards. He inspires the organisation towards achieving business goals and fulfilling the vision, mission and values by striving for continuous improvement, driving innovation and equipping the organisation to embrace change. He possesses excellent analytical, problem-solving and leadership skills and is an effective people leader.',\n",
       " 'positive': 'The Chief Executive Officer (CEO) is responsible for establishing the long-term strategic vision to enhance the growth of the organization in alignment with its core values and mission. This role involves translating overarching objectives into actionable plans, proactively identifying emerging trends, and capitalizing on new business opportunities. The CEO serves as the primary representative of the organization to clients, investors, and partners, while also ensuring a commitment to workplace safety, health, and compliance with industry quality standards. By fostering a culture of continuous improvement and innovation, the CEO motivates the organization to achieve its goals and fulfill its vision. Strong analytical, problem-solving, and leadership capabilities are essential, as well as the ability to effectively lead and inspire teams.',\n",
       " 'negative': 'The Chief Executive Officer (CEO) of a Non-Profit Organization is tasked with defining the long-term strategic initiatives to enhance community outreach in line with the organization’s mission and values. This position involves translating general objectives into specific community programs, predicting and responding to social trends, and leveraging partnerships for funding opportunities. The CEO represents the organization to stakeholders, donors, and community leaders, while being responsible for promoting a culture of volunteer engagement and compliance with regulatory standards. By driving community involvement and fostering innovation, the CEO encourages the organization to meet its outreach goals and fulfill its mission. Exceptional communication, problem-solving, and leadership skills are crucial, along with the ability to effectively manage a diverse group of volunteers.\\n\\n## Reason\\nThe negative description differs from the anchor by focusing on a non-profit context rather than a corporate one, with responsibilities centered around community outreach instead of business growth. The job title is the same, but the role emphasizes volunteer engagement and community programs rather than business strategy and operational management. The job role is Chief Executive Officer (CEO) in a Non-Profit Organization.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['valid'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_wandb",
   "metadata": {},
   "source": [
    "## Initialize Weights & Biases and Model Configuration\n",
    "\n",
    "We initialize Weights & Biases for experiment tracking and define our base model and save path. We're using the `all-minilm-l6-v2` model as our starting point, which is a efficient, general-purpose sentence embedding model that provides a good balance between speed and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "981a6573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">all-minilm-l6</strong> at: <a href='https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes/runs/xfr0589u' target=\"_blank\">https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes/runs/xfr0589u</a><br> View project at: <a href='https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes' target=\"_blank\">https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250821_145635-xfr0589u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/fatinnurafiqahibrahim/Library/CloudStorage/OneDrive-CXSAnalytics/CXS_Code_Analysis_Fatin/Bert_Finetuning/Distilabel/wandb/run-20250821_145651-fwqbcm1n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes/runs/fwqbcm1n' target=\"_blank\">all-minilm-l6</a></strong> to <a href='https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes' target=\"_blank\">https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes/runs/fwqbcm1n' target=\"_blank\">https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes/runs/fwqbcm1n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fatinnurafiqah-imk-cxsanalytics/rag-datakit-finetunes/runs/fwqbcm1n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x177bc2210>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "model_id = \"sentence-transformers/all-minilm-l6-v2\"\n",
    "save_model_path = \"./models/all-minilm-l6-v2\"\n",
    "\n",
    "# wandb.login()\n",
    "wandb.init(project=\"rag-datakit-finetunes\", name=\"all-minilm-l6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_args",
   "metadata": {},
   "source": [
    "## Configure Training Arguments\n",
    "\n",
    "We configure the training arguments for our embedding model. These parameters control various aspects of the training process including batch sizes, learning rate, precision settings, and checkpointing behavior. Key configurations include:\n",
    "\n",
    "- 5 training epochs with cosine learning rate scheduler\n",
    "- Mixed precision training using bf16 for efficiency\n",
    "- Gradient accumulation to achieve an effective batch size of 512\n",
    "- NO_DUPLICATES batch sampler to ensure diverse negative samples\n",
    "- Checkpointing at the end of each epoch with a limit of 3 saved models\n",
    "- Evaluation after each epoch to monitor validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4efaba0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your setup doesn't support bf16/gpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m args = \u001b[43mSentenceTransformerTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# number of epochs\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# train batch size\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# for a global batch size of 512\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# evaluation batch size\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\u001b[38;5;66;43;03m# warmup ratio\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# learning rate, 2e-5 is a good value\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# use cosine learning rate scheduler\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madamw_torch_fused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# use fused adamw optimizer\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf32\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                                  \u001b[49m\u001b[38;5;66;43;03m# use tf32 precision\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                                  \u001b[49m\u001b[38;5;66;43;03m# use bf16 precision\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_sampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBatchSamplers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNO_DUPLICATES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# evaluate after each epoch\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# save after each epoch\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# log after each epoch\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# save only the last 3 models\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# load the best model when training ends\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwandb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:138\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, prompts, batch_sampler, multi_dataset_batch_sampler, router_mapping, learning_rate_mapping)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages/sentence_transformers/training_args.py:247\u001b[39m, in \u001b[36mSentenceTransformerTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_sampler = (\n\u001b[32m    250\u001b[39m         BatchSamplers(\u001b[38;5;28mself\u001b[39m.batch_sampler) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.batch_sampler, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_sampler\n\u001b[32m    251\u001b[39m     )\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.multi_dataset_batch_sampler = (\n\u001b[32m    253\u001b[39m         MultiDatasetBatchSamplers(\u001b[38;5;28mself\u001b[39m.multi_dataset_batch_sampler)\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.multi_dataset_batch_sampler, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multi_dataset_batch_sampler\n\u001b[32m    256\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/TA/lib/python3.13/site-packages/transformers/training_args.py:1729\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1727\u001b[39m                     error_message += \u001b[33m\"\u001b[39m\u001b[33m You need Ampere+ GPU with cuda>=11.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1728\u001b[39m                 \u001b[38;5;66;03m# gpu\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1729\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[32m   1731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16:\n\u001b[32m   1732\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt most one of fp16 and bf16 can be True, but not both\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Your setup doesn't support bf16/gpu."
     ]
    }
   ],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=save_model_path,\n",
    "    num_train_epochs=5,                         # number of epochs\n",
    "    per_device_train_batch_size=32,             # train batch size\n",
    "    gradient_accumulation_steps=16,             # for a global batch size of 512\n",
    "    per_device_eval_batch_size=16,              # evaluation batch size\n",
    "    warmup_ratio=0.1,                           # warmup ratio\n",
    "    learning_rate=2e-5,                         # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    tf32=False,                                  # use tf32 precision\n",
    "    bf16=True,                                  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"epoch\",                      # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",                      # save after each epoch\n",
    "    logging_strategy=\"epoch\",                   # log after each epoch\n",
    "    save_total_limit=3,                         # save only the last 3 models\n",
    "    load_best_model_at_end=True,                # load the best model when training ends\n",
    "    report_to=\"wandb\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_init",
   "metadata": {},
   "source": [
    "## Initialize Model, Loss Function, and Trainer\n",
    "\n",
    "We initialize our sentence transformer model and configure the training components:\n",
    "\n",
    "- Load the base `all-minilm-l6-v2` model from sentence-transformers\n",
    "- Configure `MultipleNegativesRankingLoss` which is ideal for triplet training as it maximizes the similarity between anchor and positive pairs while minimizing similarity between anchor and negative pairs\n",
    "- Set up the `SentenceTransformerTrainer` with our model, training arguments, datasets, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07719cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050bb08a18864e29a85261a0f711015a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(model_id)\n",
    "train_loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'],  \n",
    "    loss=train_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_execution",
   "metadata": {},
   "source": [
    "## Execute Model Training\n",
    "\n",
    "We start the training process using our configured trainer. The model will train for 5 epochs, with evaluation happening after each epoch. The training progress and metrics are tracked through Weights & Biases, showing both training and validation loss metrics.\n",
    "\n",
    "As training progresses, we can observe the validation loss decreasing, indicating that our model is learning to distinguish between semantically similar and dissimilar job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade95f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.019654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.010993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.008991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.008272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.008129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.05468480090300242, metrics={'train_runtime': 20.756, 'train_samples_per_second': 363.269, 'train_steps_per_second': 0.723, 'total_flos': 0.0, 'train_loss': 0.05468480090300242, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_saving",
   "metadata": {},
   "source": [
    "## Save and Upload Trained Model\n",
    "\n",
    "After training is complete, we save the fine-tuned model to disk and upload it to Weights & Biases for versioning and sharing. The saved model includes all necessary components:\n",
    "\n",
    "- Model weights and configuration\n",
    "- Tokenizer files\n",
    "- Pooling layer configuration\n",
    "- Normalization components\n",
    "\n",
    "This ensures that the model can be easily loaded and used for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337c6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 16 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/vocab.txt',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/tokenizer_config.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/model.safetensors',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/sentence_bert_config.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/2_Normalize',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/README.md',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/tokenizer.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/checkpoint-9',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/config_sentence_transformers.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/checkpoint-12',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/config.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/training_args.bin',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/modules.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/1_Pooling',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/checkpoint-15',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/special_tokens_map.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "wandb.save(os.path.join(save_model_path, \"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Training Results and Next Steps\n",
    "\n",
    "The training completed successfully with a final validation loss of 0.00813, showing that our model has learned to effectively distinguish between semantically similar and dissimilar job descriptions. The Weights & Biases dashboard provides detailed metrics and visualizations of the training process.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To use this model in production:\n",
    "\n",
    "1. **Load the model** using `SentenceTransformer('./models/all-minilm-l6-v2')`\n",
    "2. **Evaluate** on a test set to verify performance on unseen data\n",
    "3. **Deploy** in your RAG pipeline for improved job description matching\n",
    "4. **Publish** to the Hugging Face Hub (uncomment the last cell) to share with the community\n",
    "\n",
    "The fine-tuned model is now ready to provide more accurate semantic similarity scores for job descriptions in your retrieval-augmented generation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f61a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>▃▇█▁▅</td></tr><tr><td>eval/samples_per_second</td><td>▅▂▁█▄</td></tr><tr><td>eval/steps_per_second</td><td>▅▂▁█▄</td></tr><tr><td>train/epoch</td><td>▁▃▅▅▆██</td></tr><tr><td>train/global_step</td><td>▁▃▅▅▆██</td></tr><tr><td>train/grad_norm</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.00813</td></tr><tr><td>eval/runtime</td><td>0.4424</td></tr><tr><td>eval/samples_per_second</td><td>852.164</td></tr><tr><td>eval/steps_per_second</td><td>54.249</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>15</td></tr><tr><td>train/grad_norm</td><td>0.48494</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0675</td></tr><tr><td>train_loss</td><td>0.05468</td></tr><tr><td>train_runtime</td><td>20.756</td></tr><tr><td>train_samples_per_second</td><td>363.269</td></tr><tr><td>train_steps_per_second</td><td>0.723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">all-minilm-l6</strong> at: <a href='https://wandb.ai/dnth/rag-datakit-finetunes/runs/qsq6z6xp' target=\"_blank\">https://wandb.ai/dnth/rag-datakit-finetunes/runs/qsq6z6xp</a><br> View project at: <a href='https://wandb.ai/dnth/rag-datakit-finetunes' target=\"_blank\">https://wandb.ai/dnth/rag-datakit-finetunes</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 16 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250819_143233-qsq6z6xp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0349fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.push_to_hub(\"dnth/ssf-retriever-modernbert-embed-base\", exist_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
